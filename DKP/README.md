## DKP: Deep Image Prior with Degradation Kernel Prior (DIPDKP)

Deep blind super-resolution with an explicit degradation kernel prior. The project estimates both the high-resolution (HR) image and the unknown blur kernel from a single low-resolution (LR) input. It supports multiple kernel sources:
- mcmc: Metropolis-like sampling over Gaussian/motion kernels with a kernel-prior network warm-up.
- eb: Empirical Bayes optimization of a parametric anisotropic Gaussian kernel (rotation, anisotropy, subpixel shifts) via direct gradient-based optimization.
- resnet: Extract a kernel-like prior from ResNet first-layer features (experimental).

### Key ideas
- A DIP generator (`net_dip`) creates the HR estimate directly from fixed noise.
- A kernel prior network (`net_kp`) or a differentiable parametric kernel (EB) generates a blur kernel used to degrade the HR estimate; the result is matched to the LR image.
- Two strategies for kernel estimation:
  - MCMC: random sampling of kernels with a warm-up stage to train `net_kp` to match sampled kernels, then inner-loop updates guided by image/data consistency.
  - EB: directly optimize a small set of interpretable kernel parameters (eigenvalues, rotation, shifts) using the LR/HR consistency objective with optional prior regularization

## Environment
- Python 3.8+
- PyTorch (CUDA recommended)
- numpy, matplotlib, scipy, pillow, torchvision, opencv-python, tqdm, tensorboard, openpyxl, xlwt

Install common dependencies (adjust CUDA version as needed):
```bash
pip install torch torchvision torchaudio
pip install numpy matplotlib scipy pillow opencv-python tqdm tensorboard openpyxl xlwt
```

## Project structure
- `DIPDKP/main.py`: entry point and CLI; loops over dataset images.
- `DIPDKP/model/model.py`: core training for DIP + kernel estimation (MCMC/EB/ResNet).
- `DIPDKP/model/kernel_generate.py`: kernel utilities including random Gaussian/motion kernels and `EmpiricalBayesKernel`.
- `DIPDKP/model/networks.py`: DIP and kernel-prior networks.
- `DIPDKP/model/util.py`: IO, metrics, helpers.
- `DIPDKP/Settings.py`: experiment defaults (loops, warm-up counts, logging).
- `data/prepare_dataset.py`: generate synthetic LR and GT kernels.
- Outputs under `data/log_DIPDKP/...` include per-iteration SR and kernel images, CSV logs, and event files.

## Data
Expected dataset layout (example):
```
data/datasets/Set5/HR/*.png
data/datasets/Set5/DIPDKP_lr_x4/*.png          # LR inputs (can be generated)
data/datasets/Set5/DIPDKP_gt_k_x4/*.mat        # optional GT kernels for eval
```

Generate synthetic LR/GT-kernel from HR (optional):
```bash
python data/prepare_dataset.py --dataset Set5 --sf 4 --noise_ker 0 --noise_im 0
```

## Usage
Run over the default `butterfly` sample (see `main.py` loops). Adjust flags as needed.

### MCMC kernel source
```bash
python DIPDKP/main.py --kernel_source mcmc --max_iters 1000
```
Notes:
- Uses `conf.kernel_first_iteration` warm-up (default 200) to train `net_kp` against randomly sampled kernels.
- Inner loops controlled by `I_loop_x` and `I_loop_k` inside `main.py`/`Settings.py`.

### Empirical Bayes (EB) kernel source
```bash
python DIPDKP/main.py \
  --kernel_source eb \
  --eb_steps 25 \
  --eb_lr 1e-4 \
  --eb_prior_weight 1e-4 \
  --max_iters 1000
```
Notes:
- EB does not perform MCMC warm-up; any “MCMC warm up complete” print is cosmetic and can be removed by moving the print inside the MCMC branch in `model.py`.
- EB optimizes a differentiable anisotropic Gaussian kernel with parameters: eigenvalues (positivity via softplus), rotation, and bounded subpixel shifts.

### ResNet kernel source (experimental)
```bash
python DIPDKP/main.py --kernel_source resnet --max_iters 1000
```

## Important arguments
- `--kernel_source {mcmc,resnet,eb}`: choose kernel estimation method.
- `--max_iters`: outer iterations.
- EB-specific:
  - `--eb_steps`: optimization steps per EB call.
  - `--eb_lr`: Adam LR for EB parameters.
  - `--eb_prior_weight`: weight of inverse-eigenvalue penalty.

Defaults for loop counts, warm-up, and printing are set in `Settings.py` and `main.py`:
- `conf.kernel_first_iteration=200` (MCMC warm-up)
- `I_loop_x=5`, `I_loop_k=3`, `D_loop=5` (see `main.py`)

## Outputs
Under `data/log_DIPDKP/<dataset>_DIPDKP_lr_x<sf>_<method>/`:
- `<img>_<iter>.png`: SR snapshots.
- `<img>_kernel_<iter>.png`: kernel snapshots.
- `<img>.csv`: per-iteration PSNR / reconstruction loss / kernel PSNR.
- `events.out.tfevents*`: TensorBoard logs.

## How it works (brief)
1) `net_dip` predicts HR from fixed noise. 2) A kernel is generated by either `net_kp` (trained via MCMC-guided losses) or EB’s parametric kernel. 3) HR is blurred and downsampled to reconstruct LR; losses (SSIM early, then MSE) update networks (and EB parameters if used).

## Known issues / tips
- If you see “MCMC warm up complete” under EB, it is a print location issue; move the print inside the MCMC-only branch in `model.py`.
- For EB stability, tune `--eb_steps`, `--eb_lr`, and `--eb_prior_weight`.
- For larger images, consider cropping (DIV2K path in `main.py`).

## Citation
If you use this code, please cite relevant works on DIP and kernel priors.
